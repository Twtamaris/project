{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment as a 3x3 grid\n",
    "# 0 represents empty, 1 represents obstacle, and 2 represents the goal\n",
    "env = np.array([[0, 0, 1],\n",
    "                [0, 1, 0],\n",
    "                [0, 0, 2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Q-table with zeros\n",
    "q_table = np.zeros((3, 3, 4))  # Add an extra dimension for actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.9\n",
    "num_episodes = 1000\n",
    "\n",
    "# Define actions: 0 = Up, 1 = Down, 2 = Left, 3 = Right\n",
    "actions = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24713955917566066"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# yesle 0, 1 ko bitch ko random vaule dinxa\n",
    "np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[(1,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(q_table[(1,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 2]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env[(0,2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Q-table:\n",
      "[[[0.77167185 0.56553907 0.76751952 0.9       ]\n",
      "  [0.86679915 1.         0.76222581 0.94766524]\n",
      "  [0.         0.         0.         0.        ]]\n",
      "\n",
      " [[0.76289112 0.         0.01384161 0.1       ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]]\n",
      "\n",
      " [[0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "# Q-learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the environment\n",
    "    state = (0, 0)  # Starting position\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Choose an action based on epsilon-greedy policy\n",
    "        if np.random.rand() < 0.1:  # Exploration\n",
    "            action = np.random.choice(actions)\n",
    "        else:  # Exploitation\n",
    "            action = np.argmax(q_table[state])\n",
    "\n",
    "        # Perform the action and observe the next state and reward\n",
    "        if action == 0:  # Move Up\n",
    "            next_state = (state[0] - 1, state[1])\n",
    "        elif action == 1:  # Move Down\n",
    "            next_state = (state[0] + 1, state[1])\n",
    "        elif action == 2:  # Move Left\n",
    "            next_state = (state[0], state[1] - 1)\n",
    "        else:  # Move Right\n",
    "            next_state = (state[0], state[1] + 1)\n",
    "\n",
    "        # Check if the next state is within the grid boundaries\n",
    "        if next_state[0] < 0 or next_state[0] >= env.shape[0] or \\\n",
    "           next_state[1] < 0 or next_state[1] >= env.shape[1]:\n",
    "            # Invalid move, stay in the current state\n",
    "            next_state = state\n",
    "\n",
    "        reward = env[next_state]\n",
    "\n",
    "        # Update the Q-value using the Bellman equation\n",
    "        q_table[state][action] = (1 - learning_rate) * q_table[state][action] + \\\n",
    "                                 learning_rate * (reward + discount_factor * np.max(q_table[next_state]))\n",
    "\n",
    "        # Transition to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if the goal is reached or the episode is finished\n",
    "        if reward == 2 or reward == 1:\n",
    "            done = True\n",
    "\n",
    "# Print the learned Q-table\n",
    "print(\"Learned Q-table:\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]]),\n",
       " array([[0, 0, 1],\n",
       "        [0, 1, 0],\n",
       "        [0, 0, 2]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((3, 3, 4)),env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFor state (0, 0):\\n\\nAction 0 (up) has a Q-value of 0.68550222.\\nAction 1 (down) has a Q-value of 0.53843436.\\nAction 2 (left) has a Q-value of 0.78495493.\\nAction 3 (right) has a Q-value of 0.9.\\nThe highest Q-value is 0.9, which corresponds to action 3 (right). Therefore, from state (0, 0), the machine will most likely follow action 3 (right).\\n\\nSimilarly, we can find the actions with the highest Q-values for the other states:\\n\\nFor state (0, 1):\\n\\nAction 0 (up) has a Q-value of 0.83898563.\\nAction 1 (down) has a Q-value of 1.0.\\nAction 2 (left) has a Q-value of 0.76697739.\\nAction 3 (right) has a Q-value of 0.91137062.\\nThe highest Q-value is 1.0, which corresponds to action 1 (down). Therefore, from state (0, 1), the machine will most likely follow action 1 (down).\\n\\nFor state (0, 2):\\n\\nAll actions have Q-values of 0.0.\\nIn this case, all the Q-values are the same (0.0), which means there is no preference for any action from state (0, 2). The machine may choose any action with equal probability, or the learning process is not complete yet.\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "For state (0, 0):\n",
    "\n",
    "Action 0 (up) has a Q-value of 0.68550222.\n",
    "Action 1 (down) has a Q-value of 0.53843436.\n",
    "Action 2 (left) has a Q-value of 0.78495493.\n",
    "Action 3 (right) has a Q-value of 0.9.\n",
    "The highest Q-value is 0.9, which corresponds to action 3 (right). Therefore, from state (0, 0), the machine will most likely follow action 3 (right).\n",
    "\n",
    "Similarly, we can find the actions with the highest Q-values for the other states:\n",
    "\n",
    "For state (0, 1):\n",
    "\n",
    "Action 0 (up) has a Q-value of 0.83898563.\n",
    "Action 1 (down) has a Q-value of 1.0.\n",
    "Action 2 (left) has a Q-value of 0.76697739.\n",
    "Action 3 (right) has a Q-value of 0.91137062.\n",
    "The highest Q-value is 1.0, which corresponds to action 1 (down). Therefore, from state (0, 1), the machine will most likely follow action 1 (down).\n",
    "\n",
    "For state (0, 2):\n",
    "\n",
    "All actions have Q-values of 0.0.\n",
    "In this case, all the Q-values are the same (0.0), which means there is no preference for any action from state (0, 2). The machine may choose any action with equal probability, or the learning process is not complete yet.\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
